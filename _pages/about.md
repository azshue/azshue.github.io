---
permalink: /
title: ""
excerpt:
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About me     
   I am a fifth-year Ph.D. candidate in the Department of Computer Science at the University of Maryland, College Park (UMD). As a research assistant at UMD, I work with my Ph.D. advisor Dr. [Tom Goldstein](https://www.cs.umd.edu/~tomg/) on research topics relevent to AI/ML safety. My research interest is to develop trustworthy machine learning (AI/ML) models. I have worked on research projects related to model robustness, explainability, and generalization to distribution shifts.     
   
   I have experience in studying these problems in multiple domains, including computer vision (3D vision), large language models (LLMs) and multimodal (vision-language) learning. During my Ph.D., I have interned at Nvidia, Salesforce, and Google as research intern.     
   
   Prior to UMD, I obtained my bachelor's degree in information security at the University of Science and Technology of China (USTC) in June 2019.
  
  <!-- :point_right: <span style="background-color: #cce6ff">I'm graduating in Spring 2024 and I'm on the job market for full-time roles in industrial research.</span> -->

## Selected Publications

  *For the full list of my publications, please refer to my [google scholar](https://scholar.google.com/citations?user=WPYkxjgAAAAJ&hl=en) page*

  * **On the Exploitability of Instruction Tuning**    
    **M. Shu**, J. Wang, C. Zhu, J. Geiping, C. Xiao, T. Goldstein    
    *Under review*    
    [[Pre-print](https://arxiv.org/pdf/2306.17194.pdf)] [[Code](https://github.com/azshue/AutoPoison)]


  * **On the Reliability of Watermarks for Large Language Models**    
    J. Kirchenbauer\*, J. Geiping\*, Y. Wen, **M. Shu**, K. Saifullah, K. Kong, K. Fernando, A. Saha, M. Goldblum, T. Goldstein    
    *Under review*    
    [[Pre-print](https://arxiv.org/pdf/2306.04634.pdf)] [[Code](https://github.com/jwkirchenbauer/lm-watermarking)]


  * **Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models**    
    **M. Shu**, W. Nie, D.A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, C. Xiao    
    *NeurIPS 2022*     
    [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/5bf2b802e24106064dc547ae9283bb0c-Paper-Conference.pdf)] [[Code](https://github.com/azshue/TPT)] [[Project page](https://azshue.github.io/TPT)]

  * **Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability**    
    R. Levin\*, **M. Shu\***, E. Borgnia\*, F. Huang, M. Goldblum, T. Goldstein    
    *NeurIPS 2022*     
    [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/6450ea28ebbc8437bc38775157818172-Paper-Conference.pdf)] [[Code](https://github.com/azshue/parameter-space-saliency)]

  * **The Close Relationship Between Contrastive Learning and Meta-Learning**    
    R. Ni\*, **M. Shu\***, H. Souri, M. Goldblum, T. Goldstein    
    *ICLR 2022*    
    [[Paper](https://openreview.net/pdf?id=gICys3ITSmj)] [[Code](https://github.com/RenkunNi/MetaContrastive)]

  
  * **Encoding Robustness to Image Style via Adversarial Feature Perturbation**    
    **M. Shu**, Z. Wu, M. Goldblum, T. Goldstein    
    *NeurIPS 2021*     
    [[Paper](https://proceedings.neurips.cc/paper/2021/file/ec20019911a77ad39d023710be68aaa1-Paper.pdf)] [[Code](https://github.com/azshue/AdvBN)]

  * **Adversarial Differentiable Data Augmentation for Autonomous Systems**    
    **M. Shu**, Y. Shen, M.C. Lin, T. Goldstein    
    *ICRA 2021*     
    [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561205)] [[Code](https://github.com/azshue/adversarial_data_augmentation)]
  
  * **Model-Agnostic Hierarchical Attention for 3D Object Detection**    
    **M. Shu**, L. Xue, R. Mart\'in-Mart\'in, C. Xiong, T. Goldstein, J.C. Niebles, R. Xu.    
    *Under review*     
    [[Pre-print](https://arxiv.org/pdf/2301.02650.pdf)]



  